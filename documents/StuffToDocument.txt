https://github.com/benranco


SNPpipeline

LepMAP2: 
  x- LepMAP2pipeline.sh, 
  x- tabulate_OrderMarkers_results.R / remove_duplicate_markers-VERSION2.R, 
  x- convert_csv_to_linkage.R, convert_csv_to_linkage_alreadyHorizontal.R?)

xLPmerge (LPmergeScript.R, ???) 

xCircos

createStripChartFromLGs.R, mybeanplot.R, createStripChartFromLGs-beeswarm.R

FilterForIsabel pipeline (on her machine):
  - filterForIsabel.sh
  - filterForIsabel_part1.R
  - getDepthStats-parallel.R
  - filterForIsabel_part3.R
  - filterForIsabel_part4.R

- what is ncdf/netcdf used for again? - I think it was for David Dunn.
----------------------
Small scripts:

removeRows.R
scratch10, scratch10_2
extractRowsFromCsvForJunJun.R
combineCsvTablesByColumn.R
getDepthStats-parallel.R
filter46sampleData.R (for Jun-Jun)
filterForHolly.R
MyCountClustScript.R, MyHeatmapScript.R

----------------------
Helpful:

LepMAP2:
LepMAP2-testInfo.txt, LepMAP2-testOutput.txt, LepMAP2-usageInfo.txt, 

summaryOfTasks.txt 
VCFdata-troubleshooting.txt
exampleData.txt
temp.txt

Documentation in git docs folder:
- RScriptsSummary.txt, SNPpipeline-overview, SNPpipeline-process.txt, samtools-Help.txt, freebayes-Help.txt, bowtie2-Help.txt




==============
Snippet of some possible scenarios for LepMAP2pipeline documentation:
- skip the Filtering module because you've determined Filtering isn't needed
- skip Filtering, SeparateChromosomes and JoinSingles because you've already run them and you just want to rerun OrderMarkers to see if you get better results (the OrderMarkers results are slightly different each time)
- skip Filtering, SeparateChromosomes and JoinSingles because you've finished a complete run of the LepMAP2 pipeline, including the OrderMarkers module, and you've run the tabulate_OrderMarkers_results.R script on the output, and you now want to do a second pass of the OrderMarkers module on the output of the first pass. In this case you should also make sure you use the new map file generated by tabulate_OrderMarkers_results.R in addition to the new .csv data file.

- put parameter_name: in comments


==============






OLD CONTINUE HERE: 
- convert .csv data to fasta, read firefox tab:
  https://stat.ethz.ch/pipermail/r-help/2011-August/288579.html
- convert these fasta files to fastq
- run through realphy and csi phylogeny
- read up on firefox tabs:
  http://www.mybiosoftware.com/phylosnp-take-snp-data-files-csv-and-vcf-and-generate-phylogenetic-trees.html
  https://www.ncbi.nlm.nih.gov/pubmed/24930720

- use the phylogenetic tool "mega" to analyse the new generated fasta file. (also try RealPhy and CSI Phylogeny). With a small sample fasta file first.




=======================================



x- check on progress of SNPpipeline run of Jun-Jun's 46 sample dataset (new report gen)
x- check-in code to fix check.names bug, and make sure code in SNPpipelineDevel and SNPpipelineDevel-fluidigm is up-to-date
x- test and check-in latest code to: choose which half of the pipeline to run, which optional reports to generate, to run the new depth stats report, and the new required folder for depth files.
- separate large ref file into chunks, and run SNPpipeline on a chunk (jun-jun's new SNP piplines for his 46-sample data set, using his two larger reference files rather than the 14706 ref)
- continue updating code in SNPpipelineDevel-fluidigm 



- research grdi and programs for cleaning raw data and assembling genomes sent by Jun-Jun
- check on grace pipeline, remodify code back, and check in update to getDepthStats.R




---
Old ideas for raw data file name bug fix (already fixed):

if [[ "H5Positive_Control_2_R1.fastq.gz" == *".fastq.gz" ]]; then echo "yeah"; fi 
 
or ???:
for i in *R1.hello ; do mv "$i" "${i/-[0-9.]*.pkg/.pkg}" ; done

or (I used this one):
for file in *R1.fastq ; do mv $file $(echo $file | rev | cut -c 9- | rev)---R1.fastq; done
for file in *R2.fastq ; do mv $file $(echo $file | rev | cut -c 9- | rev)---R2.fastq; done

...and then rename them back at the end
---


Overall Todo:
x- update and rerun depth stats (investigate empty cells)
x- check on changes not staged for commit, see if there's anything wrong there
x- transfer to isabel's computer
~ follow up with Holly about desktop icon
- update SNPpipeline to:
    x- capture bowtie console alignment summary output. Also write a script to go back and rerun bowtie for each sample and save the summary.
    x- fix the bug on line 96 and 117 of start.sh (cut -c 13- assumes fastq.gz) (see above for ideas)
    x- integrate depth stats script 
    x- check-in to git the new subdirectory dataTemp/depthfiles required for depth stats script
    x- enable you to choose which reports to run
    x- run only pre-reports, or run only reports
    x- fix row names = true for mutation report (I set it to true?)
    ~ investigate why SNPpipeline is only doing A/A for some (maybe NA originally?) cells, and just A/ for others. (seems to be related to whether input to freebayes is haploid or diploid)
    ~ find out how the vcf_to_tab is choosing whether to display the ALT or the REF. Does it just take the larger
  of RO and AO, or has that already been decided by freebayes when it created the .vcf, if the -p (default
  ploidy) is (1) haploid (Jun-Jun) as opposed to (2) diploid (Isabel).
    ~ investigate, can parallel_process.R be sped up by traversing down each col first rather than across?
    ~ introduce R parallelism where possible (see getDepthStats-parallel.R as an example)
    x- write a script to load all required R libraries before beginning (to run both in standalone as sudo and in the pipeline)
x- run SNPpipeline on Jun-Jun's new data
x- update all my scripts that use read.table or read.csv to specify check.names=FALSE (to prevent from converting dashes into periods), and make sure the code doesn't use col names as variables (see report_subset.R in latest SNPpipeline code).
~ organize scripts into folders, with example input/ouput data files
- install second hard drive
- finish thoroughly documenting stuff
x- ?(emailed myself on Dec 6:) Fix rownames=TRUE for mutation report

Todo:
x- using MAF_cutoff_report, for each sample (col) go back to the sam or bam file, and find out, for each position (row/cell), how many of the most ocurring [A,T,C,G] for jun-jun's, and top two occuring for isabel.

- Q: why does the .sam file not have an entry for LPss160665_c2_seq1_m.97765 with position 122 while the samtools depth output and the VCF (even the _cutoff) file both do?


x- create .csv file that replaces each data cell with DP, and another that replaces them with DP;DPB;RO;AO
x- check to see if NA cells have data in the .sam and/or the unfiltered vcf files (if it's the same as the ref, there should be data, otherwise it'll be 0) 


To parallelize getDepthStats:
- subset it into one .Rds per column, saved to a folder
- for each .Rds, do what's in getDepthStats.R, save it to a new .Rds in another folder
- read in all the .Rds's and combine them into one table


=========================
.sam: raw data after being aligned
.bam: compressed version of .sam
_sorted.bam: as name suggests
_sorted.bam.bai: index for sorted .bam
.vcf: after finding SNPs with freebayes
.recode.vcf: after removing indels
_cutoff: after filtering out all data whose QUAL field is less than 5 (still in vcf format)
.tab: after converting the _cutoff file to a tab-delimited text file listing the actual variants instead of ALT indexes.


=========================
Row extracted from Jun-Jun's VCF _cutoff file for:
cat MI.M03992_0050.001.FLD0289.Fluidigm_301_cutoff | grep "LPss160665"

This row corresponds to the first row in the MAF_cutoff.csv file:
LPss160665_c2_seq1_m.97765	122	.	G	A	2224.35	PASS	AB=0;ABP=0;AC=1;AF=1;AN=1;AO=127;CIGAR=1X;DP=165;DPB=165;DPRA=0;EPP=3.0274;EPPR=3.0103;GTI=0;LEN=1;MEANALT=1;MQM=35.7795;MQMR=35.5789;NS=1;NUMALT=1;ODDS=512.177;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=4263;QR=1176;RO=38;RPL=0;RPP=278.787;RPPR=85.5263;RPR=127;RUN=1;SAF=64;SAP=3.0274;SAR=63;SRF=19;SRP=3.0103;SRR=19;TYPE=snp	GT:DP:DPR:RO:QR:AO:QA:GL	1:165:165,127:38:1176:127:4263:-249.751,0

The same row broken into chunks:
LPss160665_c2_seq1_m.97765	
122	
.	
G	
A	
2224.35	
PASS	
AB=0;ABP=0;AC=1;AF=1;AN=1;AO=127;CIGAR=1X;DP=165;DPB=165;DPRA=0;EPP=3.0274;EPPR=3.0103;GTI=0;LEN=1;MEANALT=1;MQM=35.7795;MQMR=35.5789;NS=1;NUMALT=1;ODDS=512.177;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=4263;QR=1176;RO=38;RPL=0;RPP=278.787;RPPR=85.5263;RPR=127;RUN=1;SAF=64;SAP=3.0274;SAR=63;SRF=19;SRP=3.0103;SRR=19;TYPE=snp	
GT:DP:DPR:RO:QR:AO:QA:GL	
1:165:165,127:38:1176:127:4263:-249.751,0


AB=0;ABP=0;AC=1;AF=1;AN=1;AO=127;
CIGAR=1X;
DP=165;DPB=165;DPRA=0;
EPP=3.0274;EPPR=3.0103;GTI=0;
LEN=1;
MEANALT=1;MQM=35.7795;MQMR=35.5789;
NS=1;NUMALT=1;
ODDS=512.177;
PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;
QA=4263;QR=1176;
RO=38;RPL=0;RPP=278.787;RPPR=85.5263;RPR=127;RUN=1;
SAF=64;SAP=3.0274;SAR=63;SRF=19;SRP=3.0103;SRR=19;
TYPE=snp	


NS		1= "Number of samples with data"
***DP		165= "Total read depth at the locus"
***DPB		165= "Total read depth per bp at the locus; bases in reads overlapping / bases in haplotype"
***AC		1= "Total number of alternate alleles in called genotypes"
***AN		1= "Total number of alleles in called genotypes"
AF		1= "Estimated allele frequency in the range (0,1]"
***RO		38= "Reference allele observation count, with partial observations recorded fractionally"
***AO		127= "Alternate allele observations, with partial observations recorded fractionally"
PRO		0= "Reference allele observation count, with partial observations recorded fractionally"
PAO		0= "Alternate allele observations, with partial observations recorded fractionally"
QR		1176= "Reference allele quality sum in phred"
QA		4263= "Alternate allele quality sum in phred"
PQR		0= "Reference allele quality sum in phred for partial observations"
PQA		0= "Alternate allele quality sum in phred for partial observations"
SRF		19= "Number of reference observations on the forward strand"
SRR		19= "Number of reference observations on the reverse strand"
SAF		64= "Number of alternate observations on the forward strand"
SAR		63= "Number of alternate observations on the reverse strand"
SRP		3.0103= "Strand balance probability for the reference allele: Phred-scaled upper-bounds estimate of the probability of observing the deviation between SRF and SRR given E(SRF/SRR) ~ 0.5, derived using Hoeffding's inequality"
SAP		3.0274= "Strand balance probability for the alternate allele: Phred-scaled upper-bounds estimate of the probability of observing the deviation between SAF and SAR given E(SAF/SAR) ~ 0.5, derived using Hoeffding's inequality"
AB		0= "Allele balance at heterozygous sites: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous"
ABP		0= "Allele balance probability at heterozygous sites: Phred-scaled upper-bounds estimate of the probability of observing the deviation between ABR and ABA given E(ABR/ABA) ~ 0.5, derived using Hoeffding's inequality"
RUN		1= "Run length: the number of consecutive repeats of the alternate allele in the reference genome"
RPP		278.787= "Read Placement Probability: Phred-scaled upper-bounds estimate of the probability of observing the deviation between RPL and RPR given E(RPL/RPR) ~ 0.5, derived using Hoeffding's inequality"
RPPR		85.5263= "Read Placement Probability for reference observations: Phred-scaled upper-bounds estimate of the probability of observing the deviation between RPL and RPR given E(RPL/RPR) ~ 0.5, derived using Hoeffding's inequality"
RPL		0= "Reads Placed Left: number of reads supporting the alternate balanced to the left (5') of the alternate allele"
RPR		127= "Reads Placed Right: number of reads supporting the alternate balanced to the right (3') of the alternate allele"
EPP		3.0274= "End Placement Probability: Phred-scaled upper-bounds estimate of the probability of observing the deviation between EL and ER given E(EL/ER) ~ 0.5, derived using Hoeffding's inequality"
EPPR		3.0103= "End Placement Probability for reference observations: Phred-scaled upper-bounds estimate of the probability of observing the deviation between EL and ER given E(EL/ER) ~ 0.5, derived using Hoeffding's inequality"
DPRA		0= "Alternate allele depth ratio.  Ratio between depth in samples with each called alternate allele and those without."
ODDS		512.177= "The log odds ratio of the best genotype combination to the second-best."
GTI		0= "Number of genotyping iterations required to reach convergence or bailout."
TYPE		snp= "The type of allele, either snp, mnp, ins, del, or complex."
CIGAR		1X= "The extended CIGAR representation of each alternate allele, with the exception that '=' is replaced by 'M' to ease VCF parsing.  Note that INDEL alleles do not have the first matched base (which is provided by default, per the spec) referred to by the CIGAR."
***NUMALT		1= "Number of unique non-reference alleles in called genotypes at this position."
***MEANALT		1= "Mean number of unique non-reference allele observations per sample with the corresponding alternate alleles."
***LEN		1= "allele length"
MQM		35.7795= "Mean mapping quality of observed alternate alleles"
MQMR		35.5789= "Mean mapping quality of observed reference alleles"
PAIRED		1= "Proportion of observed alternate alleles which are supported by properly paired read fragments"
PAIREDR		1= "Proportion of observed reference alleles which are supported by properly paired read fragments"
MIN		= "Minimum depth in gVCF output block."
END		= "Last position (inclusive) in gVCF output record."




=========================
Row extracted from one of Isabel's VCF _cutoff files for:
cat HI.3518.007.Index_2.9146_cutoff | grep "PSME_00002779-RA_M31375"

This row corresponds to the first row in the MAF_cutoff.csv file:
PSME_00002779-RA_M31375	479	.	C	T	0.19723	Qual	AB=0;ABP=0;AC=2;AF=1;AN=2;AO=294;CIGAR=1X;DP=600;DPB=600;DPRA=0;EPP=7.26461;EPPR=4.53303;GTI=0;LEN=1;MEANALT=2;MQM=1.2483;MQMR=1.21992;NS=1;NUMALT=1;ODDS=3.06915;PAIRED=1;PAIREDR=0.995851;PAO=0;PQA=0;PQR=0;PRO=0;QA=10316;QR=8682;RO=241;RPL=155;RPP=4.90111;RPPR=15.3453;RPR=139;RUN=1;SAF=66;SAP=196.847;SAR=228;SRF=63;SRP=122.171;SRR=178;TYPE=snp	GT:DP:DPR:RO:QR:AO:QA:GL	1/1:600:600,294:241:8682:294:10316:-6.09436,-134.679,0


PSME_00002779-RA_M31375	
479	
.	
C	
T	
0.19723	
Qual	
AB=0;ABP=0;AC=2;AF=1;AN=2;AO=294;CIGAR=1X;DP=600;DPB=600;DPRA=0;EPP=7.26461;EPPR=4.53303;GTI=0;LEN=1;MEANALT=2;MQM=1.2483;MQMR=1.21992;NS=1;NUMALT=1;ODDS=3.06915;PAIRED=1;PAIREDR=0.995851;PAO=0;PQA=0;PQR=0;PRO=0;QA=10316;QR=8682;RO=241;RPL=155;RPP=4.90111;RPPR=15.3453;RPR=139;RUN=1;SAF=66;SAP=196.847;SAR=228;SRF=63;SRP=122.171;SRR=178;TYPE=snp	
GT:DP:DPR:RO:QR:AO:QA:GL	
1/1:600:600,294:241:8682:294:10316:-6.09436,-134.679,0



AB=0;ABP=0;AC=2;AF=1;AN=2;AO=294;
CIGAR=1X;
DP=600;DPB=600;DPRA=0;
EPP=7.26461;EPPR=4.53303;
GTI=0;
LEN=1;
MEANALT=2;MQM=1.2483;MQMR=1.21992;NS=1;
NUMALT=1;
ODDS=3.06915;
PAIRED=1;PAIREDR=0.995851;PAO=0;PQA=0;PQR=0;PRO=0;
QA=10316;QR=8682;
RO=241;RPL=155;RPP=4.90111;RPPR=15.3453;RPR=139;RUN=1;
SAF=66;SAP=196.847;SAR=228;SRF=63;SRP=122.171;SRR=178;
TYPE=snp	

===========================

In each of these, the number before the | is from Jun-Jun's data, and the number after the | is from Isabel's.

NS		1 | 1= "Number of samples with data"
***DP		165 | 600= "Total read depth at the locus"
***DPB		165 | 600= "Total read depth per bp at the locus; bases in reads overlapping / bases in haplotype"
***AC		1 | 2= "Total number of alternate alleles in called genotypes"
***AN		1 | 2= "Total number of alleles in called genotypes"
AF		1 | 1= "Estimated allele frequency in the range (0,1]"
***RO		38 | 241= "Reference allele observation count, with partial observations recorded fractionally"
***AO		127 | 294= "Alternate allele observations, with partial observations recorded fractionally"
PRO		0 | 0= "Reference allele observation count, with partial observations recorded fractionally"
PAO		0 | 0= "Alternate allele observations, with partial observations recorded fractionally"
QR		1176 | 8682= "Reference allele quality sum in phred"
QA		4263 | 10316= "Alternate allele quality sum in phred"
PQR		0 | 0= "Reference allele quality sum in phred for partial observations"
PQA		0 | 0= "Alternate allele quality sum in phred for partial observations"
SRF		19 | 63= "Number of reference observations on the forward strand"
SRR		19 | 178= "Number of reference observations on the reverse strand"
SAF		64 | 66= "Number of alternate observations on the forward strand"
SAR		63 | 228= "Number of alternate observations on the reverse strand"
SRP		3.0103 | 122.171= "Strand balance probability for the reference allele: Phred-scaled upper-bounds estimate of the probability of observing the deviation between SRF and SRR given E(SRF/SRR) ~ 0.5, derived using Hoeffding's inequality"
SAP		3.0274 | 196.847= "Strand balance probability for the alternate allele: Phred-scaled upper-bounds estimate of the probability of observing the deviation between SAF and SAR given E(SAF/SAR) ~ 0.5, derived using Hoeffding's inequality"
AB		0 | 0= "Allele balance at heterozygous sites: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous"
ABP		0 | 0= "Allele balance probability at heterozygous sites: Phred-scaled upper-bounds estimate of the probability of observing the deviation between ABR and ABA given E(ABR/ABA) ~ 0.5, derived using Hoeffding's inequality"
RUN		1 | 1= "Run length: the number of consecutive repeats of the alternate allele in the reference genome"
RPP		278.787 | 4.90111= "Read Placement Probability: Phred-scaled upper-bounds estimate of the probability of observing the deviation between RPL and RPR given E(RPL/RPR) ~ 0.5, derived using Hoeffding's inequality"
RPPR		85.5263 | 15.3453= "Read Placement Probability for reference observations: Phred-scaled upper-bounds estimate of the probability of observing the deviation between RPL and RPR given E(RPL/RPR) ~ 0.5, derived using Hoeffding's inequality"
RPL		0 | 155= "Reads Placed Left: number of reads supporting the alternate balanced to the left (5') of the alternate allele"
RPR		127 | 139= "Reads Placed Right: number of reads supporting the alternate balanced to the right (3') of the alternate allele"
EPP		3.0274 | 7.26461= "End Placement Probability: Phred-scaled upper-bounds estimate of the probability of observing the deviation between EL and ER given E(EL/ER) ~ 0.5, derived using Hoeffding's inequality"
EPPR		3.0103 | 4.53303= "End Placement Probability for reference observations: Phred-scaled upper-bounds estimate of the probability of observing the deviation between EL and ER given E(EL/ER) ~ 0.5, derived using Hoeffding's inequality"
DPRA		0 | 0= "Alternate allele depth ratio.  Ratio between depth in samples with each called alternate allele and those without."
ODDS		512.177 | 3.06915= "The log odds ratio of the best genotype combination to the second-best."
GTI		0 | 0= "Number of genotyping iterations required to reach convergence or bailout."
TYPE		snp | snp= "The type of allele, either snp, mnp, ins, del, or complex."
CIGAR		1X | 1X= "The extended CIGAR representation of each alternate allele, with the exception that '=' is replaced by 'M' to ease VCF parsing.  Note that INDEL alleles do not have the first matched base (which is provided by default, per the spec) referred to by the CIGAR."
***NUMALT		1 | 1= "Number of unique non-reference alleles in called genotypes at this position."
***MEANALT		1 | 2= "Mean number of unique non-reference allele observations per sample with the corresponding alternate alleles."
***LEN		1 | 1= "allele length"
MQM		35.7795 | 1.2483= "Mean mapping quality of observed alternate alleles"
MQMR		35.5789 | 1.21992= "Mean mapping quality of observed reference alleles"
PAIRED		1 | 1= "Proportion of observed alternate alleles which are supported by properly paired read fragments"
PAIREDR		1 | 0.995851= "Proportion of observed reference alleles which are supported by properly paired read fragments"
MIN		 | = "Minimum depth in gVCF output block."
END		 | = "Last position (inclusive) in gVCF output record."









***DP		165 | 600= "Total read depth at the locus"
***DPB		165 | 600= "Total read depth per bp at the locus; bases in reads overlapping / bases in haplotype"
***AC		1 | 2= "Total number of alternate alleles in called genotypes"
***AN		1 | 2= "Total number of alleles in called genotypes"
AF		1 | 1= "Estimated allele frequency in the range (0,1]"
***RO		38 | 241= "Reference allele observation count, with partial observations recorded fractionally"
***AO		127 | 294= "Alternate allele observations, with partial observations recorded fractionally"
***NUMALT		1 | 1= "Number of unique non-reference alleles in called genotypes at this position."
***MEANALT		1 | 2= "Mean number of unique non-reference allele observations per sample with the corresponding alternate alleles."
***LEN		1 | 1= "allele length"



==============================================

y064 - Last Thursday at 9:03 AM
felt so tired in looking prices :grinning: , begin to find and research which coin that have a potency to be used in real world in the near future, not just for trading/speculative commodity :grinning:(edited)
there are several platform that i think maybe have a potency to be used in real world: bluzelle, nimiq, dfinity, zilliqa, orchid protocol, theta token(edited)
but thats only my subjective oppinion
y064 - Last Thursday at 9:34 PM
@balance, all those project, i think, solve real problem that already exist, have strong team/advisor, and relatively short time to mainnet launch, but maybe i am wrong :sweat_smile: :grinning:
for example theta token, going to provide decentralized CDN for video streaming, platform like youtube and twitch will become potential user, also founder of youtube and twitch become their advisor
bluzelle, going to create decentralized database (more like oracle not just data cloud), the team claimed it will be more secure and faster than traditional cloud database

Rocketh - Last Saturday at 10:42 AM
@wilhelm no not selling any DGD, as I'm in for longterm rewards
Unless it gets really crazy
My bet is that there will be an enormous pump once the first rewards distribution is on the horizon. That could be still 6+ months away though
balance - Last Saturday at 5:16 PM
Yes. Might be good strategy. Me, for instance, don't even know what this distribution is. So, enough room for the potential growth. What this distribution is, btw?
Rocketh - Last Saturday at 5:57 PM
The DAO is collecting fres from onchain-Digix transactions. Those collected fees are distributed among the holders of DGD when they participate (vote) for proposals for the DAO
But because there is no governance model yet, the collected fees are going to accumulate until the community is able to vote on the first proposals. So the first proposals are going to be super rewarding to vote on
But there is also a downside potential. If the launch of Digix is unsucscessful and not a lot of people buy Digix, investors of DGD are going to be disillusioned and the price drops hard
balance - Last Saturday at 5:59 PM
Ah, I see. Interesting aspect.
Would you like to add this insight into Santiment.net platform?
Rocketh - Last Saturday at 6:02 PM
If it's not too time-consuming, sure

balance - Last Saturday at 7:00 PM
Looks like we are entering 'capitulation' phase
If you have done cash free, might be good idea to place some 'fishing' orders
Market might easily overreact and give amazingly good entry prices
As the finally some fear is coming into the market - it's getting time to rebuild positions
Imo

charlesng - Last Sunday at 6:09 AM
@balance lol.... some of my tokens actually return back to buy price after witnessing 10X ......
It was just a sight seeing ride lol
I feel that it is hard to gather enough momentum like in 1st jan to 10th jan where all coins go ATH
Maybe need another few years
The ordinary Koreans students and household already starts to lose interest
It is sad i didnt identify the top
balance - Last Sunday at 6:18 AM
Some coins will make new ATH. There have been some nice 5 waves patterns to see
But probably not all, yes.

Rocketh - Last Sunday at 11:57 AM
Guys. It you are qualified as an accredited investor, you should seriously have a look into "hadera hashgraph". The last time I was blown away this much only 2 times before. First time Bitcoin, second time Ethereum
They are going to directly compete with Ethereum by using Solidity. They have already solved all problems of Ethereum. They are like 3 years ahead
https://m.youtube.com/watch?v=OS9AALj0kHA
Mohak - Last Sunday at 12:42 PM
Hashgraph is very interesting but they are selling like 5% supply for $100-200M
Notice the range :p

ideas - Last Tuesday at 5:52 AM [March 20, 2018]
I lost interest in crypto world lately anyway... It's the Cryptowinter
Boring red and cold
balance - Last Tuesday at 5:54 AM
it's not yet winter, I'm afraid..
:wink:
probably the wave 4 "only"
but you can imagine how winter will feel like
most people will get disgusted and really depressed
moneymoney - Last Tuesday at 6:00 AM
we are in wave 4 and then after that we go to the moon right ?
balance - Last Tuesday at 6:01 AM
yep
probably, one more intensive drop to finish the wave 4
and to eliminate the rest of the weak hands
and then one and the last moon
after which the real harsh winter will come(edited)
should be much worse than what we've experience so far since January
I mean "the real winter"
Right now most long-term holders haven't been really scared (yet)
moneymoney - Last Tuesday at 6:05 AM
how can I get out before the winter  ????
you said it's like breaking up with your gf when everything is good
i dont know if I can do that :frowning:
balance - Last Tuesday at 6:06 AM
yeah.. it's hard
ad - Last Tuesday at 8:01 AM
@balance  What are the odds of wave 4 being more prolonged like wave 2 in chart above?
balance - Last Tuesday at 8:08 AM
Typically waves 2 and 4 are different.
If the waves 2 was long (in time) and complicated in shape, then wave 4 should be fast, sharp and rather simple.
May be double zigzag
Crypto Wolf - Last Tuesday at 8:43 AM
do you really think that after wave 5 we can go down so far ? back to 1B ?:tired_face:(edited)
balance - Last Tuesday at 8:58 AM
Yeah. Unfortunately
Crypto Wolf - Last Tuesday at 9:00 AM
it would mean the real burst as for  the dot com bubble :weary:
scary
y064 - Last Tuesday at 7:43 PM
@balance i can not imagine if wave 5 can bring market cap to 1 billion, thats disaster, many people will cry especially new comers who come with their hard earned money :cold_sweat: :cry: :cry:
balance - Last Tuesday at 8:57 PM
honestly, that will also be institutionals who will lose quite a lot too
balance - Last Tuesday at 9:10 PM
and my second honest opinion. unless you really know what "risks" are and to deal with them, stay away from speculations
better find the projects in crypto which are doing something in the are of your interest
and join them
in this way you will increase your chance of holding tight through the difficult stages
and accumulating at the dips
because you will understand and participate in what project is doing
this is, imo, the main benefit of crypto
it's now possible to mix the financial and professional participations
create the value together
and share the results
moneymoney - Last Tuesday at 9:22 PM
@balance but sir tether marketcap is 2b, how can whole marketcap go lower than tether ?
balance - Last Tuesday at 9:26 PM
oh. sorry, didn't pay attention to the exact number/wording
"billion" in German is "trillion" in English(edited)
moneymoney - Last Tuesday at 9:29 PM
@balance bitcoin is 9000$, can this be the end of wave 4 and beginning of wave 5 :smile: ?
charlesng - Last Tuesday at 9:35 PM
So @balance if u say u r mistaking billion and trillion from german, you mean crypto will shot ass $1 trillion in wave 5 and then it will crash hard to $1 trillion for harsh winter?
Doesnt make sense tho as $1T is definitely higher than now
Crypto Wolf - Yesterday at 1:38 AM
@charlesng#3366  EW guidelines suggests that if wave 3 is the longest then wave 5 will be same as wave 1. So probably we will see wave 5 up to 3 trillion. Then crypto winter..Usually correction reaches point 4 so we should come back to levels of MC we are today..

ideas - Yesterday at 9:06 AM
I think at some point there will be a purge (aren't we there in some way already? ) and when this happens, the fundamentals will be more important
balance - Yesterday at 9:07 AM
yes, that is likely to happen


Mohak - Last Friday at 4:52 AM
Constellation and 0chain also looked very good to me in this quarter
wilhelm - Last Friday at 4:53 AM
0chain partnered with NEO right?
Mohak - Last Friday at 4:53 AM
There's no one doing smart contracts on DAG yet, Infact all DAG ones have one or other problems, constellation or 0chain might solve it(edited)
Yes, they did.




garbonzo607 - Last Wednesday at 10:23 PM
@balance I like your approach of gauging community sentiment. Do you understand this to be like a data sample which you can extrapolate to the larger overall market? (For example, a representative sample of 900 people is enough to gauge the entire United States population within 4 percentage points, as long as the sample is representative and you can trust the participants to tell the truth.)
So for every 1 person to express a "loss of hope" sentiment, it represents 250 others?
What I don't understand is that "loss of hope" can signal a bear market. Is it wise to try and catch a falling knife? Wouldn't a better indicator be a bull sentiment very soon after the "loss of hope" comment?
So that you get the lower end of the bull market.
balance - Last Wednesday at 10:30 PM
One thing first - we are just reducing the risks of "buying hight" (in this case)
garbonzo607 - Last Wednesday at 10:30 PM
Agreed
balance - Last Wednesday at 10:30 PM
"loss of hope" never signals the beginning of the bear market
quite opposite
As the markets fall -> there is always more hope than despair
garbonzo607 - Last Wednesday at 10:31 PM
I don't necessarily think it signals the end either though :wink:
balance - Last Wednesday at 10:31 PM
That's correct
This is the second point -> we can never be sure
There is always risk
We just try to reduce it. Address it.
I came to the conclusion that there is no needs to be "perfectly right"
It's enough to identify approximately "buy" and "sell" zones
Once the "crowd mood" really turns from "excitement" to "loos of hope/concerns/lack of interest"
it is very likely we are in a good "buy zone"
Of cause - it depends also a lot on your experience and skills
For this kind of analyses  - your mind should be quiet and able be in "observer" position
So, that is much more complicated that just look and find "lose of hope" pattern

