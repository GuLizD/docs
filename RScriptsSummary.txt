
==========================
gen_report.sh:

call report_gen.R, with pwd as path
call report_subset.R, with pwd as path
calculate number of processors (line 5<--This is doing *2 to the number of processors, is this a bug?)
for every NON-"filled.Rds" file in reporttemp, call parallel_process.R in background with pwd and filename parameters
call report_gen_p2.R, with pwd as path, and $1 as maf cutoff

==========================
report_gen.R:
parameters: path

for all files in output/pooled
    read the file into report data frame (<--line17, it looks like if there's more than one file in the directory it reads each and reassigns report to it - this is inefficient and possibly buggy if there's more than one file (it only expects one file, which is a correct assumption based on program logic alone))
if report is not empty, rename col1 and col4 as CHROM and COMBINED
else single=TRUE (<--line33, this should really be called noPooled, because it checks/reads single even if single==FALSE)
for all files in output/single
    build report by merging the data existing in report with the data in the current file (join: adding columns) based on a match of columns "CHROM", "POS", "REF"), basically a new column is added to report for each .tab file. If single==true, accept unmatched rows that occur either in report or sr. If single!=true, the only unmatched rows that are accepted are those that occur in report but not sr.
write report to report.csv

possible optimizations:
- use something besides read.delim/read.table and write.csv/write.table that works with matrices rather than data frames (maybe something from the data.table library?)
- if I can handle the report as a matrix rather than dataframe, could I do a faster merge process? 
- could I initialize the size of report first (esp columns) to speed things up?
- remove objects (rm()) and flush memory (gc()) as soon and often as possible

==========================
report_subset.R:
parameters: path

read report.csv to report data frame
reset report to report minus first column
while loop: divide report into data frames of 1000 rows each, and save each dataframe to reporttemp/report_p<n>.Rds (serialized R object)

possible optimizations:
- instead of reading in from the csv, why not just take as input the report created by report_gen.R
- use something besides read.csv/read.table that works with matrices rather than data frames (maybe something from the data.table library?)
- would it be possible to split the report dataframe up using a vectorized method instead of a for loop? It kind of does already, so there's only one iteration per every 1000 rows.
- maybe initialize the size of the spl dataframe to 1000 or 999 before copying 
- remove objects (rm()) and flush memory (gc()) as soon and often as possible (maybe after writing to .Rds?)

==========================
parallel_process.R
parameters: path, file

read report dataframe from .Rds file (name passed in as file) in reporttemp/
if "COMBINED" isn't a column name, set s=4, otherwise set s=5
(<--line22, len is created but never used, it could be expensive too. increment is also never used)
for each row (a) in report
    if s<=num cols
        for each col (b) in row a (other than the first s cols)
            if col b in row a is NA
                fn = colname minus last 4 characters + "_sorted.bam" (<--line32, it uses substr() as if the first character is indexed at 0, but an example I saw indexes the first character at 1. Check this out. Also lines 39 and 43 use the substring method (what's the difference?) with indexing from 0 as well. Also fn is a confusing variable name, rename to filename).
                cmd = <path>/tools/samtools-1.3.1/samtools tview <path>/dataTemp/single/<fn> <path>/reference/formatted_output.fasta -d T -p \"<row a CHROM value>:<row a POS value>" 
(<--line34, the samtools call is only looking in dataTemp/single for the .bam, should it look in dataTemp/pooled for pooled data? - It skips the combined column.)
(<--line36, perhaps unimportant, but the first " is escaped with a \, and I don't think it's necessary since it's inside a '')
                out = read in the output of cmd as a dataframe (delimited by '\n') and convert it to a matrix
                if the first cell in row 1 of out == row a's REF cell value
                    if out has at least two rows
                        if the first cell in row 2 of out == "." OR ","
                            replace the value of report[a,b] with row a's REF value + "/" (<--line46, Jun-Jun mentioned replacing the "/" with a repeat of the reference character).
save the modified dataframe to reporttemp/report_p<n>_filled.Rds (serialized R object)

possible optimizations:
- comment out line 22
- again, if the dataframe could be represented as a matrix it might be faster (would have to be done in report_subset.R and report_gen.R first)
- try replacing the for loops with vectorized operations (either the outer row loop or the inner col loop). This might not make sense to do since we're doing a system call in the inner loop
- try a faster ifelse form for the conditionals?
- remove objects (rm()) and flush memory (gc()) as soon and often as possible

Specific:
- instead of if (is.na(report[a,b])), use which() to select the cols (is it faster for cols, or just rows?) in row a instead of a for loop
- or vectorize the process of taking a subset of report (or just a row) that only contains the na cells, or create a vector of the indices that contain na's for each row, or for the whole report
- instead of the inner if statements, do a simple for loop that just does the samtools call saves the results in a vector, and then uses either an apply function for the logic in the if statgements, or a vectorized approach to the condition checking (better), or uses an ifelse (better), or uses a which (better) 


from: /home/benrancourt/Documents/copiedFromGosuzombie/gosuzombieDesktop/SNPpipeline-isabel-fupansRun
report.csv:
"CHROM","POS","REF","HI.3518.007.Index_13.9525.tab","HI.3518.007.Index_15.8106.tab","HI.3518.007.Index_18.8207.tab","HI.3518.007.Index_2.9146.tab"
"PSME_00000002-RA",55,"A",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"PSME_00000002-RA",91,"T",NA,NA,NA,NA,NA,"A/",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"PSME_00000002-RA",108,"T",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
samtools commands:
./tools/samtools-1.3.1/samtools tview ./dataTemp/single/HI.3518.007.Index_13.9525_sorted.bam ./reference/formatted_output.fasta -d T -p PSME_00000002-RA:55
./tools/samtools-1.3.1/samtools tview ./dataTemp/single/HI.3518.007.Index_15.8106_sorted.bam ./reference/formatted_output.fasta -d T -p PSME_00000002-RA:55

==========================
report_gen_p2.R
parameters: path, MAF_CUTOFF

[First, brief summary of what happens to report during report_gen_p2.R:
***
- all "filled.Rds" files are combined to make report (written to filled_report.csv)
***
- if there are at least 20 sample columns in report, do cutoff editing (written to edited_report.csv):
    - remove all rows that have NA for more than half their values
    - remove all rows that have only two types of data, and NA is one of them
    - remove all rows that have only one type of data, and it isn't NA
***
- don't modify report, but use it to create the percentage snp's data set (snpp) (written to percentage_snps.csv):
    - calculate and store in snpp the totals of A, C, T, G, NA, the max (excluding NA), second_max (excluding NA), sum (excluding NA), and MAF (second_max / (second_max + max)
***
- remove all rows from report whose MAF (stored in snpp) is less than the MAF_CUTOFF (written to MAF_cutoff_report.csv), and sort the report by snpp's MAF field, highest to lowest
***
- use the reference fasta file to calculate and write mutation_percentage.csv (without modifying report):
    - name = sequence name
    - role = sequence annotation (name + trailing info)
    - snp = number of rows in report containing the current fasta name in their "CHROM" cell
    - length = number of character's in the sequence
    - percentage SNP = snp / length * 100
    - sort by percentage SNP
***
- copy report to reportc (beyond this point report itself is no longer manipulated or referenced)
- for each row of reportc:
    - [added by Ben] if there's only one value in the row, replace all occurences of it with H
    - if the two most frequent values make up MORE THAN 90% of the row, replace all occurences of the most frequent with H and all occurrences of the 2nd most frequent with A, and replace everything else with NA
    - else if the two most frequent values make up 90% or less, remove the row from reportc
    - write report to MAF_cutoff_report_chi.csv
    - (possible bug: What if they both have the same number of occurences? Do we just leave it to the sort algorithm to determine which gets H and which gets A?)
    - TODO: the way it's written, the two most frequent values must make up MORE THAN 90% of the row, and therefore they wouldn't qualify if they make up 90%. Should I change this to be inclusive of 90%? 
    - TODO/possible bug: What if either of the two most frequent values required for a majority of > 90% is NA? Do we delete the row? I have assumed so and programmed it accordingly.
***
- copy reportc to reportd (Bug: I believe it should copy report to reportd. FIXED.)
- for each column of reportd (there might be a bug here (s-1 doesn't look like it'll work for single rather than pooled runs because it doesn't have a COMBINED column). FIXED, check to make sure it's correct.):
    - if it's the COMBINED column, set file name to the existing "cutoff" file in outputTemp/pooled
    - else set file name to /outputTemp/single/<current col name - last 4 chars>_cutoff" (there might be a bug here for pooled data, because all the columns after COMBINED will look for files in the outputTemp/single folder. BUT, I don't think it's a bug because according to program logic there will only ever be one cutoff file in /outputTemp/pooled/, and /outputTemp/single/ will be empty unless doing single or both.)
    - ap <- scanVcf(fil)
    - rr <- ap$`*:*-*`$rowRanges
    - for each row in reportd, if the current cell is not NA:
        - (POSSIBLE BUG: checking characters in the below code may or may not take into account that the data is formatted like "A/A", and might not take the "/" into account. Todo: Test this. It seems like it might take this into account, which might be why it looks for 2 characters ("A/") or 3 characters ("A/G") in some cases. However, if so, this would break once I do the change Jun-Jun requested for filled_report.csv).
        - if the number of characters in the cell is 2
            - if the first character is not equal to the current row's REF value
                - set ind to the list of those names of rr that contain both this row's CHROM value and ":<this row's POS value>_"
                - if ind is not NA, set the current cell to 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][2]
                - else if ind is NA, set the current cell to NA
            - else if the first character is equal to the current row's REF value, set the current cell to 1
        - else (maybe should explicitly check if the number of chars in cell >= 3, since that's what it assumes. FIXED lower down, but check if this is correct.)
            - set ind to the list of those names of rr that contain both this row's CHROM value and ":<this row's POS value>_"
            - if ind is not NA (possible bug: there's no provision for when it is NA. FIXED, but check.)
                - if the first character in the cell is not equal to the third character
                    - set the current cell to 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][2]
                - else if the first and third character do equal each other
                    - set the current cell to 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][3]
            - if ind is NA, set the current cell to NA (this was added by Ben)
- write reportd to probability.csv      
***
]


read and merge (by rows) the split reports back into one large report dataframe
write this as filled_report.csv

# do editing for cutoffs:
if numcols > 24 (for COMBINED) or numcols > 23 (for non-COMBINED)
    (<--line52-55, variable tr is never used, and report is already a dataframe)
    (<--line56, nc is assigned the ouput of nrow(report), this is deceptively named)
    if "COMBINED" isn't a column name, set rfc=4, otherwise set rfc=5
    while loop: for each row a in report:
        (<--line70, possible off-by-one error here with rfc. Yup.)
        copy row a (minus the initial 4 or 5 cols) to variable c as a matrix (<--these are terrible variable names, makes the code hard to read)
        if more than half the values in c are NA
            report <- report[-a, ] (<--line73,81, this reassignment inside a while loop (performed for each line!) is potentially very expensive, can this operation be vectorized instead of using a loop?)
        else 
            if there are only two types of data in row a, and one of them is NA OR if there's only one type of data in row a, and it isn't NA:
                report <- report[-a, ] (<--line73,81,again, maybe expensive)
    write edited_report.csv
else not enough samples, skipping cutoff editing

# finding snp percentage per site:
# remember report now has the rows removed from editing for cutoffs  
(<--line97-116, snpp is built up by appending columns to it, which is expensive. Best to pre-allocate the size of snpp, and if possible even vectorize the copying of column data. See comment below as well.) 
if "COMBINED" isn't a column name
    set snpp <- first 3 columns of report, all rows
    set s=4
else 
    set snpp <- first 4 columns of report, all rows
    set s=5
convert snpp to a dataframe (<--line105, isn't snpp already a dataframe?)
for each row in report
    convert to matrix row a (minus the first 3 or 4 cols), grep with "A", then count how many A's, and store this in a new column in snpp, called "A"
    do all of the above for "C","T","G" as well (<--row a is converted to a matrix 5 times here!)
    do the same, but this time counting NA's and putting them in an "empty" col of snpp
    store the max value of this row's A,C,T,G col values in a "max" col of snpp
    store the 2nd max value of this row's A,C,T,G col values in a "max" col of snpp
    store the sum of this row's A,C,T,G col values in a "sum" col of snpp
    calculate MAF as second_max / (second_max + max) and store this in a "MAF" col of snpp
write snpp to percentage_snps.csv

# MAF_cutoff_report.csv and mutation_percentage.csv:
sort the snpp by the MAF field, highest to lowest
set snpp to only those rows that are above the MAF cutoff
set report to only those rows in the report whose row names are also in the snpp, and sort it to the same order as the rows in snpp (<--line124, does this also sort the report to the same order as the newly sorted snpp? It would seem so (I tested with a small df).
read in the formatted_output.fasta (<--line128, "output" is a confusing name for the fasta reference)
create a new empty dataframe called "out" (<--line129, out could be initialized to the right number of cols & rows) 
for each sequence name (sector) in the fasta file (everything following '>' up to the first space):
    copy the sequence annotation (name + trailing info) from the fasta row to the "role" column of a row in out bearing the same "name" as the fasta sequence.
    calculate how many rows in report contain the current fasta name in their "CHROM" column, and put the total in the "snp" column of the "name" row of out.
    count how many characters are in this fasta row's sequence, and put the total in the "length" column of the "name" row of out.
    divide the result of the recent "snp" calculation by the "length", and put the result in the "percentage SNP" column of the current "name" row of out. (<--line136, this shouldn't have to recalculate snp and length, just reuse the existing results)
sort the out dataframe by "percentage SNP", highest to lowest
multiply the "percentage SNP" column values by 100
write out to mutation_percentage.csv
write report to MAF_cutoff_report.csv (<--line144, this could have been done right after line124, after "set report to only those rows whose row names are also in the snpp" 

# replacing alleles with characters for chi square test:
set reportc <- report (<--line148, is this expensive to copy report to reportc, or just a reference/pointer?)
(<--line149,150, ct and cl, terrible variable names)
while loop, for each row in reportc:
    (<--line154,158,etc, the "COMINED" column would be excluded from pooled data. Is this correct? The routine for "generate probability values" seems to want to include it).
    set datap to the current row, minus the first "s" columns
    converts datap to a matrix, then uses the table function to calculate a factor of the row, sorts the output (most frequent to least frequent) and stores it in va. (<--line155, wouldn't need to convert the row to matrix if reportc was already a matrix)
    [added by Ben] if there's only one value in the row, replace all occurences of it with H (<--156-159, this was added by me, check if this was a correct interpretation of what it should do)
    if the first two values in va occur more than 90% of the time (<--line160, should this be > or >= 90? Also, what if there's only one value in va? Does this break? Seems like it. (fixed that). 
(<--Also, what if the top two values are equal, do we just leave it to the sort algorithm to determine which gets H and which gets A?)
        replace all occurences of the first value in all columns of reportc current row (minus the first "s" cols) with H (remember, in a factor, the value is the name, and the table function just returns a table that calculates the number of times the value occures, and uses the value as the name)
        replace all occurences of the 2nd value in all columns of reportc current row (minus the first "s" cols) with A (<--line158,165,166,171,172,179 should H and A be hardcoded here? probably, but double-check)
        if there are more than just the first two values in va
            replace all occurences of each remaining value in all columns of reportc current row (minus the first "s" cols) with NA
    else remove the current row from reportc (<--lin185, again, is this an expensive operation that could be vectorized?)
write reportc to MAF_cutoff_report_chi.csv
(<--TODO: the way it's written, the two most frequent values must make up MORE THAN 90% of the row, and therefore they wouldn't qualify if they make up 90%. Should I change this to be inclusive of 90%?) 
(<--TODO: What if either of the two most frequent values required for a majority of > 90% is NA? Do we delete the row? I have assumed so and programmed it accordingly. I've updated the table function and the if statements to include NAs in their calculations, rather than ignoring the NAs and coming up with false calculations because of it.)

# generate probability values:
set reportd <- reportc (<--line196, is this expensive to copy report to reportc, or just a reference/pointer?)
(<--line196, I believe it SHOULD be reportd <- report, based on the desciption in the readme.md, and the program logic in the routine. FIXED.)
for each col x in reportd (except the first s-1 cols):
    if this col is the "COMBINED" col (only occurs in pooled data) (<--202,204,214, if there are other cols after "COMBINED", it will look for files for them in the /outputTemp/single.  BUT, I don't think it's a bug because according to program logic there will only ever be one cutoff file in /outputTemp/pooled/, and /outputTemp/single/ will be empty unless doing single or both.)
(<--Also, the for loop seems designed only for the pooled scenario where the s-1'th column would be COMBINED, and would cause an error for single (FIXED this part). This might be buggy for both pooled and single then.)
        for each file in /outputTemp/pooled
            if the file (name?) contains "cutoff"
                set fil to the file path
    else set fil to: /outputTemp/single/<current col name - last 4 chars>_cutoff" 
    ap <- scanVcf(fil)
    rr <- ap$`*:*-*`$rowRanges
    for each row y in reportd
        initialize ind to NA
        if the current cell reportd[y, x] is not NA
            (<--line223,230,242,247,249,253, POSSIBLE BUG: checking characters in the below code may or may not take into account that the data is formatted like "A/A", and might not take the "/" into account. Todo: Test this. It seems like it might take this into account, which might be why it looks for 2 characters ("A/") or 3 characters ("A/G") in some cases. However, if so, this would break once I do the change Jun-Jun requested for filled_report.csv).)
            if the number of characters in the cell is 2
                if the first character is not equal to reportd[y, REF] 
                    set ind to the list of those names of rr that contain both this row's CHROM value and ":<this row's POS value>_"
                    if ind is not NA
                        reportd[y, x] <- 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][2]
                    else reportd[y, x] <- NA
                else reportd[y,x] <- 1
            else  (<--line242,247, maybe this should specify else if num chars is >= 3, since line 245 seems to assume it. DONE for 247, but check if this is right. If it needs to be done for 242, I'd also need to add an additional else clause at the end for the case when num chars < 2.)
                set ind to the list of those names of rr that contain both this row's CHROM value and ":<this row's POS value>_"
                if ind is not NA (<--line245, what if ind is NA? There's no provision for this, unlike above. DONE: I've added an else clause to deal with it (line256), but need to check if my action (setting cell to NA) is correct.)
                    if the first character in report[y,x] is not equal to the third character
                        reportd[y, x] <- 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][2]
                    else 
                        reportd[y, x] <- 10 ^ as.list(ap$`*:*-*`$GENO$GL[ind])[[1]][3]
                else reportd[y, x] <- NA (<--line256, added by me. Check to make sure this is needed.)
write reportd to probability.csv 

possible optimizations:
- see some of the (<-- comments
- in the finding snp percentage section, the snpp dataframe could be a matrix, and it could be initialized with the right number of rows in and columns (columns are added)


==========================

==========================

==========================

==========================

==========================

==========================

==========================
